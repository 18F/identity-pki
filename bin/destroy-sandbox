#!/bin/bash

#### tear down/destroy a sandbox environment ####

trap replace_db_files EXIT
set -eu

BASEPATH="$(dirname "$0")"
. "${BASEPATH}/lib/common.sh"
. "${BASEPATH}/lib/sandbox-lib.sh"

usage() {
    cat >&2 << EOM

Usage: ${0} [-ldebkxh] [ENV] (defaults to \$GSA_USERNAME if not specified)

Flags (will run all if no flags specified):
  -l : Delete ENV-idp-functions CloudFormation stack if it exists
  -z : Zero out all \${TF_ENV} Auto Scaling Groups
  -s : Stop and delete the CloudWatch-to-S3 Kinesis Firehose stream
  -d : Remove deletion/prevent_destroy/final snapshot from RDS dbs
  -b : Empties all S3 buckets with ENV in the bucket name
  -x : terraform-destroy + reset db .tf files
  -k : Delete all ENV-named S3 keys in secrets/app-secrets buckets
  -h : Detailed help

EOM
}

man_page() {
    cat >&2 << EOM

Usage: ${0} [-ldebkxh] [ENV]

Runs all necessary commands to prep and destroy an existing
sandbox environment, \${TF_ENV}. Designed to run with no
specified flags/arguments.

The tasks in this script can also be individually executed
using the following flags:

  -l : Delete the \${TF_ENV}-idp-functions CloudFormation stack,
       if it currently exists
  -z : Set Minimum, Desired, and Maximum numbers for all
       Auto Scaling Groups in \${TF_ENV} to 0
  -s : Stop and force-delete the cw-kinesis-s3 Firehose stream,
       allowing its corresponding bucket to be emptied and deleted
  -d : Remove deletion protection and prevent_destroy settings from
       app and idp databases, and set skip_final_snapshot to true
  -b : Empties all S3 buckets with \${TF_ENV} in the bucket name,
       and deletes all object versions as well.
  -x : Run \`terraform destroy -auto-approve\` against the
       environment, skipping the attempt to send a message to Slack.
  -k : Delete all S3 keys in the secrets/app-secrets buckets that
       have \${TF_ENV} in their name.
  -h : Displays this help

This script will default to using \${GSA_USERNAME} for the
value of \${TF_ENV}, unless a different one is specified
as a final argument.

EOM
  exit 0
}

replace_db_files() {
  for DB in idp app worker; do
    FILE="${APP_DIR}/${DB}.tf"
    [[ -f "${FILE}.bak" ]] && run mv ${FILE}.bak ${FILE}
  done
}


delete_lambdas() {
  if [[ $(ave aws cloudformation describe-stacks --stack-name ${LAMBDA_STACK} 2>/dev/null) ]] ; then
    ave aws cloudformation delete-stack --stack-name ${LAMBDA_STACK}
    ave aws cloudformation wait stack-delete-complete --stack-name ${LAMBDA_STACK}
  else
    echo_yellow "Stack ${LAMBDA_STACK} does not exist; skipping stack deletion."
  fi
}

zero_out_asgs() {
  for ASG in $(ave aws autoscaling describe-auto-scaling-groups \
    --query 'AutoScalingGroups[*].AutoScalingGroupName' |
    jq -r '.[]' | grep ${TF_ENV}) ; do
    ave aws autoscaling update-auto-scaling-group \
      --auto-scaling-group-name ${ASG} \
      --min-size 0 --desired-capacity 0 --max-size 0 
  done
}

allow_db_destroy() {
  for DB in idp app worker ; do
    cp ${APP_DIR}/${DB}.tf ${APP_DIR}/${DB}.tf.bak
    for TASK in prevent_destroy deletion_protection ; do
      sed -i '' -E "s/(${TASK} = )true/\1false/g" ${APP_DIR}/${DB}.tf
    done
    sed -i '' -E 's/#(skip_final_snapshot = true)/\1/g' ${APP_DIR}/${DB}.tf
  done

  local APPLY_CMD="-target=aws_db_instance.idp -target=aws_db_instance.idp-worker-jobs"
  
  local TFVARS="${PRIVATE_REPO}/vars/${TF_ENV}.tfvars"
  if [[ $(grep -E '^ *[^#]*enable_rds_idp_read_replica += true' "${TFVARS}") ]] ; then
    APPLY_CMD+=' -target=aws_db_instance.idp-read-replica[0]'
  fi
  if [[ ! $(grep -E '^ *[^#]*apps_enabled += 0' "${TFVARS}") ]] ; then
    APPLY_CMD+=' -target=aws_db_instance.default[0]'
  fi

  ave ${BASEPATH}/tf-deploy -t ${TF_ENV} app apply -auto-approve ${APPLY_CMD}
}

stop_kinesis_stream() {
  local CW_S3_STREAM="cw-kinesis-s3-${TF_ENV}-us-west-2"

  if [[ $(ave aws firehose describe-delivery-stream \
          --delivery-stream-name "${CW_S3_STREAM}" \
          --query 'DeliveryStreamDescription.DeliveryStreamStatus' \
          --output text) == "ACTIVE" ]] ; then
    ave aws firehose delete-delivery-stream \
     --delivery-stream-name "${CW_S3_STREAM}" \
     --allow-force-delete
  fi
}

empty_buckets() {
  boto3_check
  for BUCKET in $(ave aws s3 ls |grep ${TF_ENV} | awk '{print $NF}' | sort -r); do
    ave aws s3 rm s3://${BUCKET} --recursive
    ave python -c "import boto3 ;\
      session = boto3.Session() ;\
      s3 = session.resource(service_name='s3') ;\
      bucket = s3.Bucket('${BUCKET}') ;\
      bucket.object_versions.delete()"
  done
}

tf_destroy() {
  ave ${BASEPATH}/tf-deploy -t ${TF_ENV} app destroy -auto-approve
  replace_db_files
}

delete_env_keys() {
  for BUCKET in $(ave aws s3 ls | grep 'secrets.' | awk '{print $NF}') ; do
    ave aws s3api delete-objects \
        --bucket ${BUCKET} \
        --delete "$(ave aws s3api list-object-versions \
        --bucket ${BUCKET} \
        --output=json \
        --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' |
    jq --arg name "${TF_ENV}" '{Objects:[.[][]|
      select(.Key|contains($name))|{Key, VersionId}],Quiet: false}')"
  done
}

TASKS=(
  "delete_lambdas"
  "zero_out_asgs"
  "stop_kinesis_stream"
  "allow_db_destroy"
  "empty_buckets"
  "tf_destroy"
  "delete_env_keys"
)

TODO=()
while getopts lzsdbxkh opt
do
  case $opt in
    l) TODO+=("${TASKS[0]}") ;;
    z) TODO+=("${TASKS[1]}") ;;
    s) TODO+=("${TASKS[2]}") ;;
    d) TODO+=("${TASKS[3]}") ;;
    b) TODO+=("${TASKS[4]}") ;;
    x) TODO+=("${TASKS[5]}") ;;
    k) TODO+=("${TASKS[6]}") ;;
    h) man_page ;;
    *) usage && exit 1 ;;
  esac
done
shift $((OPTIND-1))
initialize ${1:-}

LAMBDA_STACK="${TF_ENV}-idp-functions"

run_tasks
