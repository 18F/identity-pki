#!/bin/bash

#### Tear down/destroy a sandbox environment ####

trap replace_db_files EXIT

set -euo pipefail

BASEPATH="$(dirname "$0")"
. "${BASEPATH}/lib/common.sh"
. "${BASEPATH}/lib/sandbox-lib.sh"

usage() {
    cat >&2 << EOM

Usage: ${0} [-d[a]sbkxh] [ENV] (defaults to \$GSA_USERNAME if not specified)

Flags (will run all if no flags specified):
  -d : Remove deletion protection from RDS dbs + zero out ASGs via targeted apply
  -a : Same as -d, but do full (non-targeted) apply (not run by default)
  -c : Promote any replica AuroraDB clusters to standalone clusters
  -s : Stop and delete the CloudWatch-to-S3 Kinesis Firehose stream and Logs stream
  -p : Delete the DS record for the pivcac.ENV.identitysandbox.gov hosted zone
  -b : Empties all S3 buckets with ENV in the bucket name
  -x : terraform-destroy + reset db .tf files
  -k : Delete all ENV-named S3 keys in secrets/app-secrets buckets
  -h : Detailed help

EOM
}

help_me() {
    cat >&2 << EOM

Usage: ${0} [-d[a]sbkxh] [TF_ENV]

Runs all necessary commands to prep and destroy an existing
sandbox environment, \${TF_ENV}. Designed to run with no
specified flags/arguments.

The tasks in this script can also be individually executed
using the following flags:

  -d : Remove deletion protection/prevent_destroy/final_snapshot from RDS databases,
       and zero out Auto Scaling Groups, via targeted \`tf-deploy \${TF_ENV} app apply\`.
  -a : Same as -d but will do a full \`apply\` without specifically targeting the
       RDS databases/ASGs (use in case the targeted \`apply\` is failing).
  -c : Find any/all replica AuroraDB clusters within the environment, and promote
       each one to a standalone instance (to allow deletion by Terraform)
  -s : Stop and force-delete the cw-kinesis-s3 Firehose stream and associated CloudWatch
       log stream, allowing their corresponding bucket to be emptied and deleted
  -p : Delete the DS record from identitysandbox.gov that enables DNSSEC for the
       pivcac.\${TF_ENV}.identitysandbox.gov (necessary before disabling DNSSEC)
  -b : Empties all S3 buckets with \${TF_ENV} in the bucket name,
       and deletes all object versions as well
  -x : Run \`terraform destroy -auto-approve\` against the
       environment, skipping the attempt to send a message to Slack
  -k : Delete all S3 keys in the secrets/app-secrets buckets that
       have \${TF_ENV} in their name
  -h : Displays this help

This script will default to using \${GSA_USERNAME} for the
value of \${TF_ENV}, unless a different one is specified
as a final argument.

EOM
  exit 0
}

replace_db_files() {
  for TF_FILE in 'app/idp' 'app/app' 'app/worker' 'modules/rds_aurora/main' ; do
    FILE="$(git rev-parse --show-toplevel)/terraform/${TF_FILE}.tf"
    if [[ -f "${FILE}.bak" ]] ; then
      mv ${FILE}.bak ${FILE}
    fi
  done
}

1_allow_db_destroy() {
  local TF_ARGS=()
  if [[ ! ${APPLY_ALL} == true ]] ; then
    TF_ARGS=($(ave ${BASEPATH}/tf-deploy ${TF_ENV} app state list |
      grep 'db_instance\|autoscaling_group\|rds_cluster\.'))
    TF_ARGS+=('aws_route53_record.pivcac_zone_ds') # delete pivcac parent DS
    TF_ARGS=( "${TF_ARGS[@]/#/-target=}" ) # adds DBs and ASGs as targets
  fi
  ave ${BASEPATH}/tf-deploy -z -t ${TF_ENV} app apply -auto-approve "${TF_ARGS[@]:-}"
}

2_promote_aurora_cluster() {
  for CLUSTER in $(ave aws rds describe-db-clusters |
      jq --arg TFE "${TF_ENV}" -r '.DBClusters[]|select((.DBClusterIdentifier|
      contains($TFE)) and .ReplicationSourceIdentifier != null).DBClusterIdentifier') ; do
    ave aws rds promote-read-replica-db-cluster \
      --db-cluster-identifier "${CLUSTER}"
    sleep 1
    ave aws rds wait db-cluster-available --db-cluster-identifier "${CLUSTER}"
  done
}

3_stop_kinesis_stream() {
  local CW_S3_STREAM="cw-kinesis-s3-${TF_ENV}-us-west-2"
  local CW_S3_LOG="/aws/kinesisfirehose/${CW_S3_STREAM}"

  if [[ $(ave aws firehose describe-delivery-stream \
          --delivery-stream-name "${CW_S3_STREAM}" \
          --query 'DeliveryStreamDescription.DeliveryStreamStatus' \
          --output text) == "ACTIVE" ]] ; then
    ave aws firehose delete-delivery-stream \
     --delivery-stream-name "${CW_S3_STREAM}" \
     --allow-force-delete
  fi
  sleep 5
  while [[ $(ave aws firehose describe-delivery-stream \
          --delivery-stream-name "${CW_S3_STREAM}" \
          --query 'DeliveryStreamDescription.DeliveryStreamStatus' \
          --output text) == "DELETING" ]] ; do
    sleep 5
  done

  if [[ ! -z $(ave aws logs describe-log-streams \
          --log-group-name "${CW_S3_LOG}" | jq -r '.logStreams[]') ]] ; then
    ave aws logs delete-log-stream \
      --log-group-name "${CW_S3_LOG}" --log-stream-name 'S3Delivery'
  fi
  while [[ ! -z $(ave aws logs describe-log-streams \
          --log-group-name "${CW_S3_LOG}" | jq -r '.logStreams[]') ]] ; do
    sleep 5
  done

  boto3_check
  local CACHE_BUCKET=$(ave aws s3 ls | grep "log-cache-${TF_ENV}" | awk '{print $NF}')
  empty_bucket_with_versions ${CACHE_BUCKET}
}

4_destroy_pivcac_ds_records() {
  local ZONE_ID PIVCAC_DS
  while [[ -z ${ZONE_ID} ]] ; do
    ZONE_ID=$(ave aws route53 list-hosted-zones-by-name |
              jq -r '.HostedZones[]|select(.Name == "identitysandbox.gov.").Id' |
              awk -F/ '{print $NF}')
  done
  while [[ -z ${PIVCAC_DS} ]] ; do
    PIVCAC_DS=$(ave aws route53 list-resource-record-sets \
       --hosted-zone-id $ZONE_ID |
       jq -c --arg ENV_NAME "${TF_ENV}" \
       '.ResourceRecordSets[]|select(.Type == "DS")|select(.Name|contains($ENV_NAME))')
  done
  ave aws route53 change-resource-record-sets \
      --hosted-zone-id $ZONE_ID \
      --change-batch \
      "{\"Comment\":\"DEL-pivcac.${TF_ENV}-DS\",\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\":$PIVCAC_DS}]}"
}

5_empty_buckets() {
  for BUCKET in $(ave aws s3 ls | grep ${TF_ENV} | awk '{print $NF}' | sort -r); do
    empty_bucket_with_versions ${BUCKET}
  done
}

6_tf_destroy() {
  ave ${BASEPATH}/tf-deploy -t ${TF_ENV} app destroy -auto-approve
  replace_db_files
}

7_delete_env_keys() {
  for BUCKET in $(ave aws s3 ls | grep 'secrets.' | awk '{print $NF}') ; do
    ave aws s3api delete-objects \
        --bucket ${BUCKET} \
        --delete "$(ave aws s3api list-object-versions \
        --bucket ${BUCKET} \
        --output=json \
        --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}' |
    jq --arg name "${TF_ENV}" '{Objects:[.[][]|
      select(.Key|contains($name))|{Key, VersionId}],Quiet: false}')"
  done
}

TASKS=(
  "1_allow_db_destroy"
  "2_promote_aurora_cluster"
  "3_stop_kinesis_stream"
  "4_destroy_pivcac_ds_records"
  "5_empty_buckets"
  "6_tf_destroy"
  "7_delete_env_keys"
)

TODO=()
APPLY_ALL=false
while getopts adcspbxkh opt
do
  case $opt in
    a) APPLY_ALL=true        ;;
    d) TODO+=("${TASKS[0]}") ;;
    c) TODO+=("${TASKS[1]}") ;;
    s) TODO+=("${TASKS[2]}") ;;
    p) TODO+=("${TASKS[3]}") ;;
    b) TODO+=("${TASKS[4]}") ;;
    x) TODO+=("${TASKS[5]}") ;;
    k) TODO+=("${TASKS[6]}") ;;
    h) help_me               ;;
    *) usage && exit 1       ;;
  esac
done
shift $((OPTIND-1))
initialize ${1:-}

for TF_FILE in 'app/idp' 'app/app' 'app/worker' 'modules/rds_aurora/main' ; do
  FILE="$(git rev-parse --show-toplevel)/terraform/${TF_FILE}.tf"
  cp "${FILE}" "${FILE}.bak"
  for TASK in prevent_destroy deletion_protection ; do
    sed -i '' -E "s/(${TASK} = )true/\1false/g" "${FILE}"
  done
  sed -i '' -E 's/#(skip_final_snapshot = true)/\1/g' "${FILE}"
done

if [[ ${APPLY_ALL} == true ]] ; then
  if [[ ! -z ${TODO-} ]] && [[ ! "${TODO[@]:-}" =~ "${TASKS[0]}" ]] ; then
    TODO+=("${TASKS[0]}")
  fi
fi
run_tasks
