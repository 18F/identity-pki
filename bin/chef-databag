#!/bin/bash
set -euo pipefail

# shellcheck source=/dev/null
. "$(dirname "$0")/lib/common.sh"

cd "$(dirname "$0")/.."

DATA_BAG_DIR="${DATA_BAG_DIR-"kitchen/data_bags"}"
LOCAL_S3_DIR="$DATA_BAG_DIR/users/s3"
S3_BUCKET="${S3_BUCKET-login-gov-secrets-test}"
S3_USERS_PREFIX="${S3_USERS_PREFIX-kitchen/data_bags/users}"

SKIP_S3_DOWNLOAD="${SKIP_S3_DOWNLOAD-}"

usage() {
    local basename
    basename="$(basename "$0")"
    cat >&2 <<EOM
usage: $basename COMMAND

Handle various data bag related operations for chef servers.

Data bags will be stored locally at $DATA_BAG_DIR.

When uploading user data bags, this script will automatically fetch the latest
from S3.

Environment variables that can be configured:

    \$DATA_BAG_DIR: $DATA_BAG_DIR
    \$S3_BUCKET: $S3_BUCKET
    \$S3_USERS_PREFIX: $S3_USERS_PREFIX
    \$SKIP_S3_DOWNLOAD: $SKIP_S3_DOWNLOAD

COMMANDS:


** download **

    usage: $basename download ENVIRONMENT [TYPE]

    Download data bags from ENVIRONMENT chef server.

    If TYPE is not given, download all types:
        users     The user data bags
        config    The config app data bag
        keys      The validator key and config encryption key

** upload-config **

    usage: $basename upload-config ENVIRONMENT

    Upload config data bags for ENVIRONMENT

** upload-users-from-s3 **

    usage: $basename upload-users-from-s3 ENVIRONMENT

    Download user databags from S3 \$S3_BUCKET and upload them to ENVIRONMENT.
    Set \$SKIP_S3_DOWNLOAD if you just want to upload whatever files are
    currently in the local s3/ folder.

** s3-download **

    usage: $basename s3-download

    Download user data bags from S3, overwriting anything in:
        $DATA_BAG_DIR/users/s3/

** s3-upload **

    usage: $basename s3-upload USER

    Upload user data bag for USER into S3.
    Source: $DATA_BAG_DIR/users/s3/USER.json

EOM
}

cmd_download() {
    local bag_type
    if [ $# -ge 1 ]; then
        bag_type="$1"
    else
        bag_type="all"
    fi

    echo_blue "Downloading $bag_type data bags from $ENV at $CHEF_HOST"

    case "$bag_type" in
        user|users)
            cmd_download_users
            ;;
        config)
            cmd_download_config
            ;;
        key|keys)
            cmd_download_keys
            ;;
        all)
            cmd_download_users
            cmd_download_config
            cmd_download_keys
            ;;
        *)
            echo_red "unknown bag type $bag_type"
            exit 1
            ;;
    esac

    echo_blue "Done downloading data bags"
}

# usage: backup_and_mkdir DIR
#
# If DIR exists, prompt the user for whether they wish to back it up.
# Upon yes, move the directory to a timestamped backup name.
# Upon no, abort.
#
# Then mkdir the DIR.
#
backup_and_mkdir() {
    local dir
    dir="$1"

    # ensure no trailing slashes
    dir="${dir%/}"

    if [ -z "$dir" ]; then
        echo_red "backup_and_mkdir: empty DIR argument"
        return 1
    fi

    backup_if_exists "$dir" --overwrite

    if [ -e "$dir" ]; then
        run rm -rv "$dir"
    fi

    run mkdir -v "$dir"
}

ssh_chef() {
    run ssh "$CHEF_HOST" "$@"
}
ssh_chef_sudo() {
    run ssh "$CHEF_HOST" sudo -H "$@"
}
ssh_chef_knife() {
    # We add -c to ensure we get root's knife.rb rather than the broken one
    # that is present in ~ubuntu but owned by root due to our boostrapping
    # cruft. This goes after all the other options because knife is weird.
    ssh_chef_sudo knife "$@" -c /root/.chef/knife.rb
}

cmd_download_users() {
    local db_dir

    db_dir="$DATA_BAG_DIR/users/$ENV"

    echo_blue "Downloading user data bags to $db_dir"

    backup_and_mkdir "$db_dir"

    users="$(ssh_chef_knife data bag show users)"

    for user in $users; do
        ssh_chef_knife data bag show -F json users "$user" \
            > "$db_dir/$user.json"
    done

    echo_blue "Done downloading user data bags"
}

cmd_download_config() {
    configs="$(ssh_chef_knife data bag show config)"
    if [ "$configs" != "app" ]; then
        echo_red >&2 "Got unexpected configs from knife data bag show config"
        echo_red >&2 "configs: '$configs'"
        return 1
    fi

    bag_path="$DATA_BAG_DIR/config/$ENV.json"

    echo_blue "Downloading config app databag to $bag_path"

    backup_if_exists "$bag_path" --overwrite

    ssh_chef_knife data bag show -F json config app \
        --secret-file "/etc/chef/encrypted_data_bag_secret" > "$bag_path"
    chmod 600 "$bag_path"

    echo_blue "Saved decrypted config data bag to $bag_path"
}

# usage: chef_sudo_download_key REMOTE_PATH LOCAL_PATH
#
# Download a file from REMOTE_PATH1 on the chef host and save it to LOCAL_PATH
# on the local machine, prompting the user to back up the target file if the
# content differs.
#
# NB: This script is not suitable for large files because it loads the file
# content into memory.
#
chef_sudo_download_key() {
    local remote_path local_path remote_content local_content
    remote_path="$1"
    local_path="$2"

    echo_blue >&2 "Downloading key from chef server:"
    echo_blue >&2 "  remote path: '$remote_path'"
    echo_blue >&2 "  local path:  '$local_path'"


    remote_content="$(ssh_chef_sudo cat "$remote_path")"

    if [ -e "$local_path" ]; then
        local_content="$(cat "$local_path")"

        if [ "$local_content" = "$remote_content" ]; then
            echo_green "Local file $local_path already matches remote file"
            run chmod 600 "$local_path"
            return
        fi

        echo_red "File already exists at '$local_path'"
        echo_red "Local content and remote content differ"

        backup_if_exists "$local_path" --overwrite
    fi

    echo "$remote_content" > "$local_path"
    echo_green "Wrote key to $local_path"

    run chmod 600 "$local_path"
}

cmd_download_keys() {
    local username

    echo_blue >&2 "Downloading chef keys for $ENV"

    # This is currently only used to set the username used to create the chef
    # user, which we don't need here. So set a placeholder to avoid needing to
    # request GSA_USERNAME from the user.
    username="${GSA_USERNAME-<PLACEHOLDER>}"

    (
    # use a subshell to ensure that load-env.sh doesn't infect variables

    echo_blue >&2 "Loading environment"
    echo >&2 "+ . bin/load-env.sh $ENV $username"
    # shellcheck source=/dev/null
    . "bin/load-env.sh" "$ENV" "$username"

   echo >&2 "Checking that some needed env variables exist"
   # shellcheck disable=SC2154
   cat <<EOM
   TF_VAR_chef_home: $TF_VAR_chef_home
   TF_VAR_chef_databag_key_path: $TF_VAR_chef_databag_key_path
EOM

   # ensure ~/.chef is 700 since it contains keys
   run chmod 0700 "$TF_VAR_chef_home"

   echo_blue >&2 "Downloading validator key"
   validator_path="$TF_VAR_chef_home/$ENV-login-dev-validator.pem"
   chef_sudo_download_key "/root/login-dev-validator.pem" "$validator_path"

   echo_blue >&2 "Downloading data bag secret key"
   chef_sudo_download_key "/etc/chef/encrypted_data_bag_secret" \
       "$TF_VAR_chef_databag_key_path"

   )

   echo_blue >&2 "Done downloading chef keys"
}

cmd_upload_config() {
    local remote_tmpdir

    bag_path="$DATA_BAG_DIR/config/$ENV.json"

    echo_blue "Uploading config data bags to $ENV at $CHEF_HOST"
    echo_blue "Source data bag path: $bag_path"

    assert_file_exists "$bag_path"

    # write data bag to remote tmpdir
    remote_tmpdir="$(ssh_chef_sudo mktemp -d)"
    ssh_chef_sudo "tee '$remote_tmpdir/config.json' >/dev/null" < "$bag_path"

    # add to chef
    ssh_chef_knife data bag from file config "$remote_tmpdir/config.json" \
        --secret-file "/etc/chef/encrypted_data_bag_secret"

    # cleanup
    ssh_chef_sudo rm -rfv "$remote_tmpdir"

    echo_blue "Done uploading config data bag"
}

cmd_upload_users_from_s3() {
    echo_blue "Uploading user data bags from S3"

    if [ -z "$SKIP_S3_DOWNLOAD" ]; then
        echo_blue "First we will download the latest user data bags from S3"
    fi

    echo_blue "Then we will upload those data bags to the given environment"

    if [ -z "$SKIP_S3_DOWNLOAD" ]; then
        cmd_s3_download
    fi

    _upload_user_data_bags "$LOCAL_S3_DIR"
}

_upload_user_data_bags() {
    local remote_tmpdir local_dir

    local_dir="$1"

    echo_blue "Uploading user data bags from $local_dir to $CHEF_HOST"

    remote_tmpdir="$(ssh_chef mktemp -d)"

    run rsync -av "$local_dir/" "$CHEF_HOST:$remote_tmpdir"

    for file in "$local_dir"/*.json; do
        ssh_chef_knife data bag from file users "$remote_tmpdir/$(basename "$file")"
    done

    ssh_chef rm -rfv "$remote_tmpdir"

    echo_blue "Done uploading user data bags"
}

cmd_s3_download() {
    echo_blue "Downloading user data bags from S3"

    # subshell for CD
    (
    cd "$DATA_BAG_DIR"

    mkdir -vp users/s3

    run aws s3 sync --delete "s3://$S3_BUCKET/$S3_USERS_PREFIX/" "users/s3/"
    )

    echo_blue "Download complete"
}

cmd_s3_upload() {
    user="$1"
    echo_blue "Uploading user data bag for $user to S3"

    # subshell for CD
    (
    cd "$DATA_BAG_DIR/users/s3/"

    run aws s3 cp "$user.json" "s3://$S3_BUCKET/$S3_USERS_PREFIX/"
    )
}

if [ $# -lt 1 ]; then
    usage
    exit 1
fi

command="$1"
shift

set_env() {
    if [ $# -lt 1 ]; then
        usage
        return 1
    fi
    ENV="$1"
    CHEF_HOST="chef.$ENV.login.gov"
}

case "$command" in
    download)
        set_env "$@"; shift
        cmd_download "$@"
        ;;
    upload-config)
        set_env "$@"; shift
        cmd_upload_config "$@"
        ;;
    upload-users-from-s3)
        set_env "$@"; shift
        cmd_upload_users_from_s3 "$@"
        ;;
    s3-download)
        if [ $# -ne 0 ]; then
            usage
            exit 1
        fi
        cmd_s3_download "$@"
        ;;
    s3-upload)
        if [ $# -ne 1 ]; then
            usage
            exit 1
        fi
        cmd_s3_upload "$@"
        ;;
    *)
        usage
        exit 1
        ;;
esac
