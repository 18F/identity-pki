<%-
#
# This file is the Test Kitchen configuration file for the per node integration
# tests.  See
# https://github.com/18F/identity-devops/blob/master/doc/technical/testing.md
# for more details.
#
# When you make a new integration test, you should make it in a directory named
# after this node's role, and then add the proper subnet, security_group and
# iam_profile below.
#

# The first statement in erb isn't allowed to be "require"
common_dir = File.dirname(File.expand_path(File.readlink(__FILE__)))
require File.join(common_dir, "cloud_init_utils", "cloud_init_builder")

require 'yaml'
require 'zlib'
require 'aws-sdk-ec2'

Region = 'us-west-2'

role = File.basename(Dir.pwd)
environment = ENV.fetch('KITCHEN_EC2_INTEGRATION_ENVIRONMENT', 'ci')

# TODO look up dynamically
ami_id = ENV.fetch('KITCHEN_EC2_AMI', 'ami-04f2b6f5ae4c9adf2')
if role == "idp" || role == "pivcac"
  ami_id = ENV.fetch('KITCHEN_RAILS_AMI', 'ami-05166097d18d1cb9c')
end

vpc_name = "login-vpc-#{environment}"

# set default AWS profile to sandbox account
ENV['AWS_PROFILE'] ||= 'identitysandbox.gov'

# TODO: Make a standard instance profile name for each instance so we don't need
# this hash.
iam_profiles = {
    "jumphost" => "#{environment}-base-permissions",
    "idp" => "#{environment}_idp_instance_profile",
    "elasticsearch" => "#{environment}_elasticsearch_instance_profile",
    "elk" => "#{environment}_elk_instance_profile",
    "app" => "#{environment}-base-permissions",
    "outboundproxy" => "#{environment}_obproxy_instance_profile",
    "pivcac" => "#{environment}_pivcac_instance_profile",
    "migration" => "#{environment}-migration",
}
if !iam_profiles.key?(role)
  raise "IAM profile for role: #{role} not found in test kitchen boilerplate.  Please add it in #{__FILE__}."
end
iam_profile = iam_profiles.fetch(role)
# TODO: Switch to filtering on the subnet's tag: "type" => "public" / "private"
subnets = {
    "jumphost" => "login-jumphost1_subnet-#{environment}",
    "idp" => "login-idp1_subnet-#{environment}",
    "pivcac" => "login-idp1_subnet-#{environment}",
    "elasticsearch" => "login-elasticsearch_subnet-#{environment}-*",
    "elk" => "login-elk_subnet-#{environment}-*",
    "app" => "login-public1_subnet-#{environment}",
    "outboundproxy" => "login-public1_subnet-#{environment}",
    "migration" => "login-idp1_subnet-#{environment}",
}
if !subnets.key?(role)
  raise "Subnet for role: #{role} not found in test kitchen boilerplate.  Please add it in #{__FILE__}."
end
subnet = subnets.fetch(role)

# By default we look up security groups based on the "role" tag
security_group_tag_name = "role"
security_group_tag_value = role

# Roles that reuse another role's group (contrary to best practice)
# TODO: move these to their own security groups
sg_steal_roles = {
  'elasticsearch' => 'elk',
}
if sg_steal_roles.include?(role)
  security_group_tag_value = sg_steal_roles.fetch(role)
end

# We want to discover real elasticsearch nodes, unless we're running the
# elasticsearch integration test.
es_tag_value = "elasticsearch"
if role == "elasticsearch"
  es_tag_value = "test-kitchen-elasticsearch"
end
# We want to discover real elk nodes, unless we're running the elk integration
# test.
elk_tag_value = "elk"
if role == "elk"
  elk_tag_value = "test-kitchen-elk"
end

# Enable the proxy unless env var is set to disable
if ENV['KITCHEN_DISABLE_PROXY'] || role == "outboundproxy"
  proxy_server = nil
  proxy_port = nil
  no_proxy_hosts = nil
else
  # duplicates the values in terraform-app/variables.tf
  proxy_server = 'obproxy.login.gov.internal'
  proxy_port = '3128'
  no_proxy_hosts = 'localhost,127.0.0.1,169.254.169.254,169.254.169.123,.login.gov.internal,s3-us-west-2.amazonaws.com,s3.us-west-2.amazonaws.com,s3.dualstack.us-west-2.amazonaws.com,ec2.us-west-2.amazonaws.com,kms.us-west-2.amazonaws.com,secretsmanager.us-west-2.amazonaws.com,ssm.us-west-2.amazonaws.com'
end

cloud_init_info = [
  # TODO: Figure out why this is still necessary here but works in production.
  {
    "filename" => "fix_volume_permissions.yaml",
    "template" => "../common/user_data/fix_volume_permissions.yaml.erb",
    "content_type" => "text/cloud-config",
    "vars" => {}
  },
  {
    "filename" => "set-hostname.yaml",
    "template" => "../../terraform-modules/bootstrap/cloud-init.hostname.yaml.erb",
    "content_type" => "text/cloud-config",
    "vars" => {
      "hostname_prefix" => role,
      "domain" => "#{environment}.login.gov"
    }
  },
  {
    "filename" => "provision.sh",
    "template" => "../../terraform-modules/bootstrap/provision.sh",
    "content_type" => "text/x-shellscript",
    "vars" => {}
  },
  {
    "filename" => "base.yaml",
    "template" => "../../terraform-modules/bootstrap/cloud-init.base.yaml.erb",
    "content_type" => "text/cloud-config",
    "vars" => {
      "domain" => "login.gov",
      "env" => "#{environment}",
      "role" => role,
      "proxy_server" => proxy_server,
      "proxy_port" => proxy_port,
      "no_proxy_hosts" => no_proxy_hosts,
    }
  },
  # run identity-devops-private provisioning using provision.sh
  {
    "filename" => "provision-private.yaml",
    "template" => "../../terraform-modules/bootstrap/cloud-init.provision.yaml.erb",
    "content_type" => "text/cloud-config",
    "vars" => {
      # assume chef is already installed in the AMI
      "chef_download_sha256" => "",
      "chef_download_url" => "",

      # we're not in an ASG, don't try to notify lifecycle hooks
      "asg_name" => "",
      "lifecycle_hook_name" => "",

      # uncomment and customize in order to test new chef versions
      #"chef_download_sha256" => "9ddcd5ceef19c95ecc1f34bef080c23d9cb42ae8ebc69fd41dcf1c768a6a708f",
      #"chef_download_url" => "https://packages.chef.io/files/stable/chef/14.13.11/ubuntu/18.04/chef_14.13.11-1_amd64.deb",

      "provision_phase_name" => "private-provisioning",

      "git_clone_url" => "git@github.com:18F/identity-devops-private",
      "git_ref" => "master",
      "s3_ssh_key_url" => "s3://login-gov.secrets.894947205914-us-west-2/common/id_ecdsa.id-do-private.deploy"
    }
  }
]

user_data = build_cloud_init(cloud_init_info)

unless ENV['KITCHEN_EC2_SSH_KEY'] && ENV['KITCHEN_EC2_SSH_KEYPAIR_ID']
  puts "ERROR: Environment variable KITCHEN_EC2_SSH_KEY not set"
  puts "You should upload an SSH public key to AWS, then set these env vars:"
  puts "KITCHEN_EC2_SSH_KEY: path to the local private key"
  puts "KITCHEN_EC2_SSH_KEYPAIR_ID: label/name of the key in AWS console"
  puts "The bin/import-ssh-key script can help do this."
  raise "KITCHEN_EC2_SSH_KEY (path to key) and KITCHEN_EC2_SSH_KEYPAIR_ID (name in AWS) not set, run bin/import-ssh-key to import a key to AWS, then set env variables in your ~/.bashrc or similar"
end

ssh_key_file = ENV.fetch('KITCHEN_EC2_SSH_KEY')
ssh_key_file = File.expand_path(ssh_key_file)
# ensure file exists because kitchen won't error when it doesn't
if !File.exist?(ssh_key_file)
  raise "SSH key file not found: #{ssh_key_file.inspect}. Override with $KITCHEN_EC2_SSH_KEY"
end

ssh_key_pair_name = ENV.fetch('KITCHEN_EC2_SSH_KEYPAIR_ID')

ec2 = Aws::EC2::Resource.new(region: Region)

# Run a number of checks that expected resources exist, because kitchen-ec2
# itself bombs out it horrible ways without giving any helpful error messages.

# get VPC
vpc = ec2.vpcs(filters: [{name: 'tag:Name', values: [vpc_name]}]).first
unless vpc
  raise "No VPC found named #{vpc_name.inspect}"
end


# Ensure ssh_key_pair_name exists
ec2.key_pair(ssh_key_pair_name).key_fingerprint

# Ensure subnet exists
if ec2.subnets(filters: [{name: 'tag:Name', values: [subnet]}]).count.zero?
  raise "No subnet named #{subnet.inspect} found"
end

def get_security_group(ec2:, vpc:, tag_key:, tag_value:)
  filters = [
    {name: "vpc-id", values: [vpc.vpc_id]},
    {name: "tag:#{tag_key}", values: [tag_value]},
  ]
  groups = ec2.security_groups(filters: filters).to_a
  if groups.empty?
    raise "No security group found with filters: #{filters.inspect}"
  end

  if groups.length > 1
    raise "Many SGs found for filters: #{filters.inspect}: #{groups.inspect}"
  end

  groups.fetch(0)
end

# Look up security group. Test kitchen only lets you filter on one key, so we
# have to do it ourselves if we want to filter on VPC ID + tag.
main_security_group = get_security_group(vpc: vpc, ec2: ec2, tag_key: security_group_tag_name, tag_value: security_group_tag_value)

base_security_group = get_security_group(vpc: vpc, ec2: ec2,  tag_key: 'Name', tag_value: "#{environment}-base")

def gzip(string)
  begin
    # ruby 2.4+
    return Zlib.gzip(string)
  rescue NoMethodError
  end

  sio = StringIO.new
  sio.binmode
  gz = Zlib::GzipWriter.new(sio)
  gz.write(string)
  gz.close
  sio.string
end

# Compress user data if it's over 16K limit
if user_data.length > 16_384
  user_data = gzip(user_data)
end

%>

<%=
{
    "driver" => {
        "name" => "ec2",
        "aws_ssh_key_id" => ssh_key_pair_name,
        "region" => Region,
        "associate_public_ip" => true, # necessary to access from outside VPC
        "subnet_filter" => {
            "tag" => "Name",
            "value" => subnet
        },
        "security_group_ids" => [main_security_group, base_security_group].map(&:group_id),
        "instance_type" => "c5.xlarge",
        "image_id" => ami_id,
        "user_data" => user_data,
        "iam_profile_name" => iam_profile,
        "tags" => {
            "created-by" => "test-kitchen",
            "user" => ENV['GSA_USERNAME'] || ENV['USER'],
            "terminate-after" => (Time.now + 604800).strftime('%F'),
            "Name" => "test-kitchen-#{role}",
            "prefix" => "test-kitchen-#{role}",
            "domain" => "#{environment}.login.gov"
        }
    },
    "transport" => {
        "username" => "ubuntu",
        "ssh_key" => ssh_key_file,
        "name" => "speedy_ssh",
        "compression" => "zlib",
        "compression_level" => 9
    },
    "lifecycle" => {
      # pre_converge commands run after creating an instance but before the
      # chef converge phase begins. We use this to block until cloud-init has
      # finished installing updates and running identity-devops-private.
      "pre_converge" => [
        {
          "remote" =>
            "while ! [ -e /var/lib/cloud/instance/boot-finished ]; do systemctl is-system-running; echo Waiting for systemd / cloud-init boot to finish; sleep 15; done"
        },
      ],
    },
    "provisioner" => {
        "name" => "chef_zero",
        "always_update_cookbooks" => "true",
        "environments_path" => "../../kitchen/environments",
        "roles_path" => "../../kitchen/roles",
        "client_rb" => {
            "environment" => "#{environment}",
            #"treat_deprecation_warnings_as_errors" => true,
            #"resource_cloning" => false,
        },
        "install_strategy" => "skip", # Assume provision.sh has installed chef
        "chef_license" => "accept", # ugh
        "root_path" => "/var/lib/kitchen"
    },
    "verifier" => {
        "name" => "inspec"
    },
    "platforms" => [{
        "name" => "ubuntu-ami",
    }],
    "suites" => [{
        "name" => "default",
        "run_list" => ["role[#{role}]"],
        "verifier" => {
            "inspec_tests" => ["test/smoke/default"]
        },
        "attributes" => {
            "build-essential" => {
                "compile_time" => true
            },
            "apt" => {
                "compile_time_update" => true
            },
            "provisioner" => {
                "name" => "kitchen-ec2",
                "auto-scaled" => true
            },
            "elk" => {
                "es_tag_key" => "prefix",
                "es_tag_value" => es_tag_value,
                "elk_tag_key" => "prefix",
                "elk_tag_value" => elk_tag_value
            }
        }
    }]
}.to_yaml
%>

# vim: set ft=eruby :
